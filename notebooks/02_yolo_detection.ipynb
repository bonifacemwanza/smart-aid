{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Notebook 2: YOLOv8 Object Detection\n\nThis notebook demonstrates **YOLO-World** object detection on captured frames.\n\n## What's Different About YOLO-World?\n- **Zero-shot detection**: Can detect ANY object by name (not limited to 80 COCO classes)\n- **Custom classes**: We can add \"door\", \"stairs\", \"obstacle\" etc.\n- **CLIP-based**: Uses language-vision model for flexible detection\n\n## What You'll Learn\n- Loading YOLO-World model with custom classes\n- Running inference on images\n- Visualizing detection results\n- Analyzing detection accuracy"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "from src.config import Config, DetectionConfig\n",
    "from src.detector import Detector, Detection\n",
    "from src.utils import Timer\n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Load YOLO-World Model\n\nUsing YOLO-World with custom classes relevant for visually impaired navigation:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Use YOLO-World for custom class detection (including doors!)\nconfig = DetectionConfig(\n    model=\"yolov8s-world.pt\",  # YOLO-World model\n    confidence=0.2,            # Lower threshold for better recall\n    classes=[\n        \"door\", \"person\", \"chair\", \"table\", \"stairs\", \n        \"wall\", \"window\", \"car\", \"bicycle\", \"obstacle\"\n    ]\n)\n\ndetector = Detector(config)\n\nprint(\"Loading YOLO-World model...\")\nstart = time.time()\nif detector.load():\n    print(f\"Model loaded in {time.time()-start:.2f}s\")\n    print(f\"Custom classes: {detector.class_names}\")\nelse:\n    print(\"Failed to load model\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List Available Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"Custom Classes (YOLO-World can detect any object by name):\")\nfor i, name in enumerate(detector.class_names):\n    print(f\"  {i:2d}: {name}\")\n\nprint(f\"\\nNote: YOLO-World uses CLIP for zero-shot detection.\")\nprint(\"You can add any class name and it will try to detect it!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Test Images\n",
    "\n",
    "Load images from captures or use sample images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "captures_dir = Path(\"../data/captures\")\n",
    "samples_dir = Path(\"../data/sample_images\")\n",
    "\n",
    "image_files = list(captures_dir.glob(\"*.jpg\")) + list(samples_dir.glob(\"*.jpg\"))\n",
    "\n",
    "if not image_files:\n",
    "    print(\"No images found. Creating a test image...\")\n",
    "    test_img = np.zeros((480, 640, 3), dtype=np.uint8)\n",
    "    test_img[:] = (128, 128, 128)\n",
    "    cv2.putText(test_img, \"Test Image\", (200, 240), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "    samples_dir.mkdir(parents=True, exist_ok=True)\n",
    "    cv2.imwrite(str(samples_dir / \"test.jpg\"), test_img)\n",
    "    image_files = [samples_dir / \"test.jpg\"]\n",
    "\n",
    "print(f\"Found {len(image_files)} images:\")\n",
    "for f in image_files[:5]:\n",
    "    print(f\"  - {f.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Detection on Single Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if image_files:\n",
    "    test_image_path = image_files[0]\n",
    "    print(f\"Processing: {test_image_path.name}\")\n",
    "    \n",
    "    frame = cv2.imread(str(test_image_path))\n",
    "    print(f\"Image shape: {frame.shape}\")\n",
    "    \n",
    "    timer = Timer(\"detection\")\n",
    "    timer.start()\n",
    "    detections = detector.detect(frame)\n",
    "    inference_time = timer.stop()\n",
    "    \n",
    "    print(f\"\\nInference time: {inference_time*1000:.1f}ms\")\n",
    "    print(f\"Detections: {len(detections)}\")\n",
    "    \n",
    "    for det in detections:\n",
    "        print(f\"  - {det.class_name}: {det.confidence:.2f} at {det.bbox}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Detection Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if image_files and frame is not None:\n",
    "    result_frame = detector.draw_detections(frame, detections)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    axes[0].imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    axes[0].set_title(\"Original Image\")\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    axes[1].imshow(cv2.cvtColor(result_frame, cv2.COLOR_BGR2RGB))\n",
    "    axes[1].set_title(f\"Detection Result ({len(detections)} objects)\")\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    results_dir = Path(\"../data/results\")\n",
    "    results_dir.mkdir(parents=True, exist_ok=True)\n",
    "    plt.savefig(results_dir / f\"detection_{test_image_path.stem}.jpg\", dpi=150)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Processing - All Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = []\n",
    "total_time = 0\n",
    "\n",
    "for img_path in image_files:\n",
    "    frame = cv2.imread(str(img_path))\n",
    "    if frame is None:\n",
    "        continue\n",
    "    \n",
    "    timer = Timer()\n",
    "    timer.start()\n",
    "    detections = detector.detect(frame)\n",
    "    elapsed = timer.stop()\n",
    "    total_time += elapsed\n",
    "    \n",
    "    all_results.append({\n",
    "        'filename': img_path.name,\n",
    "        'detections': detections,\n",
    "        'time_ms': elapsed * 1000,\n",
    "        'frame': frame\n",
    "    })\n",
    "\n",
    "print(f\"Processed {len(all_results)} images\")\n",
    "print(f\"Total time: {total_time:.2f}s\")\n",
    "print(f\"Average time: {total_time/len(all_results)*1000:.1f}ms per image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "all_classes = []\n",
    "all_confidences = []\n",
    "\n",
    "for result in all_results:\n",
    "    for det in result['detections']:\n",
    "        all_classes.append(det.class_name)\n",
    "        all_confidences.append(det.confidence)\n",
    "\n",
    "class_counts = Counter(all_classes)\n",
    "\n",
    "print(\"Detection Summary:\")\n",
    "print(f\"  Total detections: {len(all_classes)}\")\n",
    "print(f\"  Unique classes: {len(class_counts)}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "for cls, count in class_counts.most_common(10):\n",
    "    print(f\"  {cls}: {count}\")\n",
    "\n",
    "if all_confidences:\n",
    "    print(f\"\\nConfidence statistics:\")\n",
    "    print(f\"  Min: {min(all_confidences):.2f}\")\n",
    "    print(f\"  Max: {max(all_confidences):.2f}\")\n",
    "    print(f\"  Mean: {np.mean(all_confidences):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence Distribution Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_confidences:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    axes[0].hist(all_confidences, bins=20, color='steelblue', edgecolor='white')\n",
    "    axes[0].set_xlabel('Confidence')\n",
    "    axes[0].set_ylabel('Count')\n",
    "    axes[0].set_title('Detection Confidence Distribution')\n",
    "    \n",
    "    if class_counts:\n",
    "        classes = list(class_counts.keys())[:10]\n",
    "        counts = [class_counts[c] for c in classes]\n",
    "        axes[1].barh(classes, counts, color='steelblue')\n",
    "        axes[1].set_xlabel('Count')\n",
    "        axes[1].set_title('Top 10 Detected Classes')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(results_dir / \"detection_statistics.jpg\", dpi=150)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save All Detection Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for result in all_results:\n",
    "    frame = result['frame']\n",
    "    detections = result['detections']\n",
    "    filename = result['filename']\n",
    "    \n",
    "    result_frame = detector.draw_detections(frame, detections)\n",
    "    output_path = results_dir / f\"detection_{Path(filename).stem}.jpg\"\n",
    "    cv2.imwrite(str(output_path), result_frame)\n",
    "\n",
    "print(f\"Saved {len(all_results)} detection results to {results_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary\n\nThis notebook demonstrated:\n1. Loading **YOLO-World** model for zero-shot object detection\n2. Detecting custom classes like \"door\", \"stairs\", \"obstacle\"\n3. Visualizing bounding boxes and labels\n4. Analyzing detection statistics\n\n**Key Advantages of YOLO-World:**\n- Can detect objects not in COCO (doors, stairs, curbs)\n- Flexible - just add class names\n- Good for accessibility applications\n\n**Trade-offs:**\n- Slightly slower than YOLOv8n (~100-200ms vs ~30ms)\n- Lower confidence scores (use threshold ~0.2)\n- Model is larger (26MB vs 6MB)\n\n**Next:** Run notebook 03_depth_estimation.ipynb to add distance information"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Notebook 5: Full Analysis for Thesis\n\nThis notebook generates all artifacts needed for thesis presentation:\n- Comparison grids showing pipeline stages\n- Performance benchmarks\n- Statistical analysis\n- Publication-ready figures\n\n## System Overview\n- **Detection**: YOLO-World (zero-shot, custom classes)\n- **Depth**: Depth Anything V2 (monocular)\n- **Fusion**: Priority-based obstacle ranking"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import time\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "import json\n",
    "\n",
    "from src.config import Config\n",
    "from src.detector import Detector\n",
    "from src.depth import DepthEstimator\n",
    "from src.fusion import FusionEngine\n",
    "from src.utils import Timer, create_side_by_side\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 150\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "print(\"Analysis notebook ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from src.config import DetectionConfig, DepthConfig, FusionConfig\n\n# YOLO-World with custom navigation classes\ndetection_config = DetectionConfig(\n    model=\"yolov8s-world.pt\",\n    confidence=0.2,\n    classes=[\n        \"door\", \"person\", \"chair\", \"table\", \"stairs\",\n        \"wall\", \"window\", \"car\", \"bicycle\", \"obstacle\"\n    ]\n)\n\ndepth_config = DepthConfig(model=\"vits\")\nfusion_config = FusionConfig(danger_zone=1.5, warning_zone=3.0)\n\ndetector = Detector(detection_config)\ndepth_estimator = DepthEstimator(depth_config)\nfusion = FusionEngine(fusion_config)\n\nprint(\"Loading YOLO-World...\")\ndetector.load()\nprint(f\"  Classes: {detector.class_names}\")\n\nprint(\"Loading Depth Anything V2...\")\ndepth_estimator.load()\nprint(\"Models loaded!\")\n\ncaptures_dir = Path(\"../data/captures\")\nsamples_dir = Path(\"../data/sample_images\")\nresults_dir = Path(\"../data/results\")\nresults_dir.mkdir(parents=True, exist_ok=True)\n\ntimestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load All Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_files = list(captures_dir.glob(\"*.jpg\")) + list(samples_dir.glob(\"*.jpg\"))\n",
    "print(f\"Found {len(image_files)} images for analysis\")\n",
    "\n",
    "images = []\n",
    "for path in image_files:\n",
    "    img = cv2.imread(str(path))\n",
    "    if img is not None:\n",
    "        images.append({'path': path, 'frame': img, 'name': path.stem})\n",
    "\n",
    "print(f\"Loaded {len(images)} images successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process All Images and Collect Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "detection_times = []\n",
    "depth_times = []\n",
    "fusion_times = []\n",
    "\n",
    "print(\"Processing images...\")\n",
    "for i, img_data in enumerate(images):\n",
    "    frame = img_data['frame']\n",
    "    name = img_data['name']\n",
    "    \n",
    "    t0 = time.perf_counter()\n",
    "    detections = detector.detect(frame)\n",
    "    t1 = time.perf_counter()\n",
    "    depth_map = depth_estimator.estimate(frame)\n",
    "    t2 = time.perf_counter()\n",
    "    obstacles = fusion.process(detections, depth_map, frame.shape[1])\n",
    "    t3 = time.perf_counter()\n",
    "    \n",
    "    det_time = (t1 - t0) * 1000\n",
    "    dep_time = (t2 - t1) * 1000\n",
    "    fus_time = (t3 - t2) * 1000\n",
    "    \n",
    "    detection_times.append(det_time)\n",
    "    depth_times.append(dep_time)\n",
    "    fusion_times.append(fus_time)\n",
    "    \n",
    "    results.append({\n",
    "        'name': name,\n",
    "        'frame': frame,\n",
    "        'detections': detections,\n",
    "        'depth_map': depth_map,\n",
    "        'obstacles': obstacles,\n",
    "        'det_time': det_time,\n",
    "        'dep_time': dep_time,\n",
    "        'fus_time': fus_time,\n",
    "        'total_time': det_time + dep_time + fus_time\n",
    "    })\n",
    "    \n",
    "    if (i + 1) % 5 == 0:\n",
    "        print(f\"  Processed {i+1}/{len(images)}\")\n",
    "\n",
    "print(f\"\\nProcessing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\" * 60)\nprint(\"PERFORMANCE ANALYSIS\")\nprint(\"=\" * 60)\n\ntotal_times = [r['total_time'] for r in results]\n\nprint(f\"\\nDetection (YOLO-World):\")\nprint(f\"  Mean: {np.mean(detection_times):.1f}ms\")\nprint(f\"  Std:  {np.std(detection_times):.1f}ms\")\nprint(f\"  Min:  {np.min(detection_times):.1f}ms\")\nprint(f\"  Max:  {np.max(detection_times):.1f}ms\")\n\nprint(f\"\\nDepth Estimation (Depth Anything V2 - CPU):\")\nprint(f\"  Mean: {np.mean(depth_times):.1f}ms\")\nprint(f\"  Std:  {np.std(depth_times):.1f}ms\")\nprint(f\"  Min:  {np.min(depth_times):.1f}ms\")\nprint(f\"  Max:  {np.max(depth_times):.1f}ms\")\n\nprint(f\"\\nFusion:\")\nprint(f\"  Mean: {np.mean(fusion_times):.2f}ms\")\n\nprint(f\"\\nTotal Pipeline:\")\nprint(f\"  Mean: {np.mean(total_times):.1f}ms\")\nprint(f\"  FPS:  {1000/np.mean(total_times):.2f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n\ncategories = ['YOLO-World', 'Depth', 'Fusion']\nmeans = [np.mean(detection_times), np.mean(depth_times), np.mean(fusion_times)]\nstds = [np.std(detection_times), np.std(depth_times), np.std(fusion_times)]\n\nbars = axes[0].bar(categories, means, yerr=stds, capsize=5, \n                   color=['steelblue', 'coral', 'seagreen'])\naxes[0].set_ylabel('Time (ms)')\naxes[0].set_title('Processing Time by Stage')\n\naxes[1].pie(means, labels=categories, autopct='%1.1f%%',\n            colors=['steelblue', 'coral', 'seagreen'])\naxes[1].set_title('Time Distribution')\n\naxes[2].hist(total_times, bins=15, color='steelblue', edgecolor='white')\naxes[2].axvline(np.mean(total_times), color='red', linestyle='--', \n                label=f'Mean: {np.mean(total_times):.1f}ms')\naxes[2].set_xlabel('Total Time (ms)')\naxes[2].set_ylabel('Count')\naxes[2].set_title('Total Pipeline Time Distribution')\naxes[2].legend()\n\nplt.tight_layout()\nplt.savefig(results_dir / f\"performance_analysis_{timestamp}.png\")\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_detections = []\n",
    "all_obstacles = []\n",
    "\n",
    "for r in results:\n",
    "    all_detections.extend(r['detections'])\n",
    "    all_obstacles.extend(r['obstacles'])\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DETECTION STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nTotal detections: {len(all_detections)}\")\n",
    "print(f\"Total obstacles: {len(all_obstacles)}\")\n",
    "print(f\"Average detections per image: {len(all_detections)/len(results):.1f}\")\n",
    "\n",
    "if all_detections:\n",
    "    class_counts = Counter([d.class_name for d in all_detections])\n",
    "    confidences = [d.confidence for d in all_detections]\n",
    "    \n",
    "    print(f\"\\nConfidence: {np.mean(confidences):.2f} +/- {np.std(confidences):.2f}\")\n",
    "    print(f\"\\nTop 10 Classes:\")\n",
    "    for cls, count in class_counts.most_common(10):\n",
    "        print(f\"  {cls}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obstacle Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_obstacles:\n",
    "    distances = [o.distance for o in all_obstacles]\n",
    "    positions = Counter([o.position for o in all_obstacles])\n",
    "    \n",
    "    danger = sum(1 for o in all_obstacles if o.is_danger)\n",
    "    warning = sum(1 for o in all_obstacles if o.is_warning and not o.is_danger)\n",
    "    safe = len(all_obstacles) - danger - warning\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"OBSTACLE ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(f\"\\nDistance Statistics:\")\n",
    "    print(f\"  Mean: {np.mean(distances):.2f}m\")\n",
    "    print(f\"  Min:  {np.min(distances):.2f}m\")\n",
    "    print(f\"  Max:  {np.max(distances):.2f}m\")\n",
    "    \n",
    "    print(f\"\\nZone Distribution:\")\n",
    "    print(f\"  Danger (<1.5m):  {danger} ({100*danger/len(all_obstacles):.1f}%)\")\n",
    "    print(f\"  Warning (1.5-3m): {warning} ({100*warning/len(all_obstacles):.1f}%)\")\n",
    "    print(f\"  Safe (>3m):       {safe} ({100*safe/len(all_obstacles):.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nPosition Distribution:\")\n",
    "    for pos, count in positions.items():\n",
    "        print(f\"  {pos}: {count} ({100*count/len(all_obstacles):.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Comparison Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = min(4, len(results))\n",
    "samples = results[:n_samples]\n",
    "\n",
    "fig, axes = plt.subplots(n_samples, 4, figsize=(16, 4*n_samples))\n",
    "if n_samples == 1:\n",
    "    axes = axes.reshape(1, -1)\n",
    "\n",
    "for i, r in enumerate(samples):\n",
    "    frame = r['frame']\n",
    "    detections = r['detections']\n",
    "    depth_map = r['depth_map']\n",
    "    obstacles = r['obstacles']\n",
    "    \n",
    "    axes[i, 0].imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    axes[i, 0].set_title(f\"Original: {r['name']}\")\n",
    "    axes[i, 0].axis('off')\n",
    "    \n",
    "    det_frame = detector.draw_detections(frame.copy(), detections)\n",
    "    axes[i, 1].imshow(cv2.cvtColor(det_frame, cv2.COLOR_BGR2RGB))\n",
    "    axes[i, 1].set_title(f\"Detection ({len(detections)} objects)\")\n",
    "    axes[i, 1].axis('off')\n",
    "    \n",
    "    depth_colored = depth_estimator.colorize(depth_map)\n",
    "    axes[i, 2].imshow(cv2.cvtColor(depth_colored, cv2.COLOR_BGR2RGB))\n",
    "    axes[i, 2].set_title(\"Depth Map\")\n",
    "    axes[i, 2].axis('off')\n",
    "    \n",
    "    fus_frame = fusion.draw_obstacles(frame.copy(), obstacles)\n",
    "    axes[i, 3].imshow(cv2.cvtColor(fus_frame, cv2.COLOR_BGR2RGB))\n",
    "    axes[i, 3].set_title(f\"Fusion ({len(obstacles)} obstacles)\")\n",
    "    axes[i, 3].axis('off')\n",
    "\n",
    "plt.suptitle(\"Smart Aid Pipeline Comparison\", fontsize=16, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(results_dir / f\"comparison_grid_{timestamp}.png\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = {\n",
    "    'timestamp': timestamp,\n",
    "    'num_images': len(results),\n",
    "    'performance': {\n",
    "        'detection_ms': {\n",
    "            'mean': float(np.mean(detection_times)),\n",
    "            'std': float(np.std(detection_times))\n",
    "        },\n",
    "        'depth_ms': {\n",
    "            'mean': float(np.mean(depth_times)),\n",
    "            'std': float(np.std(depth_times))\n",
    "        },\n",
    "        'total_ms': float(np.mean(total_times)),\n",
    "        'fps': float(1000/np.mean(total_times))\n",
    "    },\n",
    "    'detections': {\n",
    "        'total': len(all_detections),\n",
    "        'classes': dict(Counter([d.class_name for d in all_detections]).most_common(10))\n",
    "    },\n",
    "    'obstacles': {\n",
    "        'total': len(all_obstacles),\n",
    "        'danger_count': sum(1 for o in all_obstacles if o.is_danger),\n",
    "        'warning_count': sum(1 for o in all_obstacles if o.is_warning and not o.is_danger),\n",
    "        'avg_distance': float(np.mean([o.distance for o in all_obstacles])) if all_obstacles else 0\n",
    "    }\n",
    "}\n",
    "\n",
    "summary_path = results_dir / f\"analysis_summary_{timestamp}.json\"\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"Summary saved to: {summary_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thesis-Ready Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"THESIS SUMMARY TABLE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\"\"\n",
    "| Metric                    | Value            |\n",
    "|---------------------------|------------------|\n",
    "| Images Processed          | {len(results)}              |\n",
    "| Total Detections          | {len(all_detections)}              |\n",
    "| Total Obstacles           | {len(all_obstacles)}              |\n",
    "| Detection Time (mean)     | {np.mean(detection_times):.1f} ms          |\n",
    "| Depth Time (mean)         | {np.mean(depth_times):.1f} ms          |\n",
    "| Total Pipeline (mean)     | {np.mean(total_times):.1f} ms          |\n",
    "| Effective FPS             | {1000/np.mean(total_times):.1f}              |\n",
    "| Danger Zone Detections    | {sum(1 for o in all_obstacles if o.is_danger)} ({100*sum(1 for o in all_obstacles if o.is_danger)/max(1,len(all_obstacles)):.1f}%)         |\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary\n\nThis analysis notebook has generated:\n\n1. **Performance metrics** - Timing for each pipeline stage\n2. **Detection statistics** - Class distribution, confidence scores\n3. **Obstacle analysis** - Distance zones, positions\n4. **Visual comparisons** - Grid showing pipeline stages\n5. **JSON summary** - Machine-readable results\n\nAll outputs are saved in `data/results/` for thesis inclusion.\n\n## Key Findings\n\n| Aspect | Standard YOLO | Our System (YOLO-World) |\n|--------|---------------|------------------------|\n| Door detection | No | Yes |\n| Stairs detection | No | Yes |\n| Custom classes | Limited to 80 | Unlimited |\n| Depth estimation | Requires extra sensor | Single RGB camera |\n| Cost | N/A | ~$250 |"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
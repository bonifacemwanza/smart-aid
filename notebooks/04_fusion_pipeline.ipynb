{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Notebook 4: Fusion Pipeline - Detection + Depth\n\nThis notebook combines **YOLO-World** detection with **Depth Anything V2** to create\na complete obstacle awareness system.\n\n## Pipeline Flow\n```\nFrame → YOLO-World → Detections (door, person, stairs, etc.)\n                  ↘\n                    → Fusion Engine → Obstacles with distances\n                  ↗\nFrame → Depth Anything V2 → Depth Map\n```\n\n## What You'll Learn\n- Combining detection bounding boxes with depth maps\n- Calculating distance to detected objects\n- Priority-based obstacle ranking\n- Position awareness (left/center/right)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "from src.config import Config\n",
    "from src.detector import Detector\n",
    "from src.depth import DepthEstimator\n",
    "from src.fusion import FusionEngine, Obstacle\n",
    "from src.utils import Timer, create_side_by_side\n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from src.config import Config, DetectionConfig, DepthConfig, FusionConfig\n\n# Use YOLO-World for custom class detection\ndetection_config = DetectionConfig(\n    model=\"yolov8s-world.pt\",\n    confidence=0.2,\n    classes=[\n        \"door\", \"person\", \"chair\", \"table\", \"stairs\",\n        \"wall\", \"window\", \"car\", \"bicycle\", \"obstacle\"\n    ]\n)\n\n# Depth config\ndepth_config = DepthConfig(model=\"vits\")\n\n# Fusion config\nfusion_config = FusionConfig(\n    danger_zone=1.5,\n    warning_zone=3.0\n)\n\nprint(\"Loading YOLO-World...\")\ndetector = Detector(detection_config)\ndetector.load()\nprint(f\"  Classes: {detector.class_names}\")\n\nprint(\"\\nLoading Depth Anything V2...\")\ndepth_estimator = DepthEstimator(depth_config)\ndepth_estimator.load()\nprint(\"  Done!\")\n\nfusion = FusionEngine(fusion_config)\nprint(\"\\nAll models loaded!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Test Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "captures_dir = Path(\"../data/captures\")\n",
    "samples_dir = Path(\"../data/sample_images\")\n",
    "results_dir = Path(\"../data/results\")\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "image_files = list(captures_dir.glob(\"*.jpg\")) + list(samples_dir.glob(\"*.jpg\"))\n",
    "print(f\"Found {len(image_files)} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Single Image Through Full Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if image_files:\n",
    "    test_path = image_files[0]\n",
    "    frame = cv2.imread(str(test_path))\n",
    "    print(f\"Processing: {test_path.name}\")\n",
    "    print(f\"Shape: {frame.shape}\")\n",
    "    \n",
    "    det_timer = Timer(\"detection\")\n",
    "    det_timer.start()\n",
    "    detections = detector.detect(frame)\n",
    "    det_time = det_timer.stop()\n",
    "    print(f\"\\n1. Detection: {len(detections)} objects in {det_time*1000:.1f}ms\")\n",
    "    \n",
    "    depth_timer = Timer(\"depth\")\n",
    "    depth_timer.start()\n",
    "    depth_map = depth_estimator.estimate(frame)\n",
    "    depth_time = depth_timer.stop()\n",
    "    print(f\"2. Depth: completed in {depth_time*1000:.1f}ms\")\n",
    "    \n",
    "    fusion_timer = Timer(\"fusion\")\n",
    "    fusion_timer.start()\n",
    "    obstacles = fusion.process(detections, depth_map, frame.shape[1])\n",
    "    fusion_time = fusion_timer.stop()\n",
    "    print(f\"3. Fusion: {len(obstacles)} obstacles in {fusion_time*1000:.1f}ms\")\n",
    "    \n",
    "    total_time = det_time + depth_time + fusion_time\n",
    "    print(f\"\\nTotal pipeline: {total_time*1000:.1f}ms ({1/total_time:.1f} FPS)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obstacle Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if obstacles:\n",
    "    print(\"\\nDetected Obstacles (sorted by priority):\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for i, obs in enumerate(obstacles):\n",
    "        danger = \"DANGER\" if obs.is_danger else (\"WARNING\" if obs.is_warning else \"\")\n",
    "        print(f\"\\n{i+1}. {obs.class_name.upper()}\")\n",
    "        print(f\"   Distance: {obs.distance:.2f}m {danger}\")\n",
    "        print(f\"   Position: {obs.position}\")\n",
    "        print(f\"   Confidence: {obs.confidence:.2f}\")\n",
    "        print(f\"   Priority: {obs.priority}\")\n",
    "        print(f\"   Alert: '{obs.to_alert_text()}'\")\n",
    "else:\n",
    "    print(\"No obstacles detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Full Pipeline Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if frame is not None and depth_map is not None:\n    detection_frame = detector.draw_detections(frame.copy(), detections)\n    fusion_frame = fusion.draw_obstacles(frame.copy(), obstacles)\n    depth_colored = depth_estimator.colorize(depth_map)\n    \n    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n    \n    axes[0, 0].imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n    axes[0, 0].set_title(\"1. Original Image\")\n    axes[0, 0].axis('off')\n    \n    axes[0, 1].imshow(cv2.cvtColor(detection_frame, cv2.COLOR_BGR2RGB))\n    axes[0, 1].set_title(f\"2. YOLO-World Detection ({len(detections)} objects)\")\n    axes[0, 1].axis('off')\n    \n    axes[1, 0].imshow(cv2.cvtColor(depth_colored, cv2.COLOR_BGR2RGB))\n    axes[1, 0].set_title(\"3. Depth Estimation\")\n    axes[1, 0].axis('off')\n    \n    axes[1, 1].imshow(cv2.cvtColor(fusion_frame, cv2.COLOR_BGR2RGB))\n    axes[1, 1].set_title(f\"4. Fusion Result ({len(obstacles)} obstacles)\")\n    axes[1, 1].axis('off')\n    \n    plt.suptitle(f\"Smart Aid Pipeline - {test_path.name}\", fontsize=14)\n    plt.tight_layout()\n    plt.savefig(results_dir / f\"pipeline_{test_path.stem}.jpg\", dpi=150)\n    plt.show()\nelse:\n    print(\"No depth map available - check depth estimator\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Process All Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = []\n",
    "\n",
    "for img_path in image_files:\n",
    "    frame = cv2.imread(str(img_path))\n",
    "    if frame is None:\n",
    "        continue\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    detections = detector.detect(frame)\n",
    "    depth_map = depth_estimator.estimate(frame)\n",
    "    obstacles = fusion.process(detections, depth_map, frame.shape[1])\n",
    "    \n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    all_results.append({\n",
    "        'filename': img_path.name,\n",
    "        'frame': frame,\n",
    "        'detections': detections,\n",
    "        'depth_map': depth_map,\n",
    "        'obstacles': obstacles,\n",
    "        'time_ms': elapsed * 1000,\n",
    "        'fps': 1 / elapsed\n",
    "    })\n",
    "\n",
    "avg_fps = np.mean([r['fps'] for r in all_results])\n",
    "print(f\"Processed {len(all_results)} images\")\n",
    "print(f\"Average FPS: {avg_fps:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "all_obstacles = []\n",
    "for result in all_results:\n",
    "    all_obstacles.extend(result['obstacles'])\n",
    "\n",
    "print(f\"Total obstacles detected: {len(all_obstacles)}\")\n",
    "\n",
    "if all_obstacles:\n",
    "    classes = Counter([o.class_name for o in all_obstacles])\n",
    "    positions = Counter([o.position for o in all_obstacles])\n",
    "    distances = [o.distance for o in all_obstacles]\n",
    "    \n",
    "    danger_count = sum(1 for o in all_obstacles if o.is_danger)\n",
    "    warning_count = sum(1 for o in all_obstacles if o.is_warning and not o.is_danger)\n",
    "    \n",
    "    print(f\"\\nDanger zone (<1.5m): {danger_count}\")\n",
    "    print(f\"Warning zone (1.5-3m): {warning_count}\")\n",
    "    print(f\"Safe (>3m): {len(all_obstacles) - danger_count - warning_count}\")\n",
    "    \n",
    "    print(f\"\\nPosition distribution:\")\n",
    "    for pos, count in positions.most_common():\n",
    "        print(f\"  {pos}: {count}\")\n",
    "    \n",
    "    print(f\"\\nDistance statistics:\")\n",
    "    print(f\"  Min: {min(distances):.2f}m\")\n",
    "    print(f\"  Max: {max(distances):.2f}m\")\n",
    "    print(f\"  Mean: {np.mean(distances):.2f}m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save All Pipeline Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for result in all_results:\n",
    "    frame = result['frame']\n",
    "    obstacles = result['obstacles']\n",
    "    depth_map = result['depth_map']\n",
    "    filename = result['filename']\n",
    "    \n",
    "    fusion_frame = fusion.draw_obstacles(frame, obstacles)\n",
    "    depth_colored = depth_estimator.colorize(depth_map)\n",
    "    combined = create_side_by_side(fusion_frame, depth_colored, \n",
    "                                    (\"Obstacles\", \"Depth\"))\n",
    "    \n",
    "    output_path = results_dir / f\"fusion_{Path(filename).stem}.jpg\"\n",
    "    cv2.imwrite(str(output_path), combined)\n",
    "\n",
    "print(f\"Saved {len(all_results)} fusion results to {results_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Fusion Logic\n",
    "\n",
    "### Priority Calculation\n",
    "```\n",
    "priority = distance_score + class_score + confidence_score\n",
    "\n",
    "distance_score:\n",
    "  < 1.5m (danger)  → 100\n",
    "  < 3.0m (warning) → 50\n",
    "  else             → 10\n",
    "\n",
    "class_score:\n",
    "  person, car, bicycle → +20-30\n",
    "  furniture            → +10\n",
    "  other               → 0\n",
    "\n",
    "confidence_score: confidence × 10\n",
    "```\n",
    "\n",
    "### Position Detection\n",
    "- **Left**: center_x < 33% of frame width\n",
    "- **Center**: 33% - 67%\n",
    "- **Right**: center_x > 67%\n",
    "\n",
    "### Distance Estimation\n",
    "1. Scale detection bbox to depth map resolution\n",
    "2. Extract center region of bbox\n",
    "3. Average depth values in region\n",
    "4. Convert to estimated meters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary\n\nThis notebook demonstrated the complete fusion pipeline:\n1. **YOLO-World detection** → What objects are present (including doors, stairs!)\n2. **Depth estimation** → How far are they\n3. **Fusion** → Which obstacles matter most\n\n**Key Advantages:**\n- Can detect doors, stairs, curbs (not in COCO)\n- Single RGB camera for both detection AND depth\n- Priority-based alerting for most critical obstacles\n\n**Performance (MacBook M1/M2):**\n- YOLO-World: ~100-200ms\n- Depth: ~800-1200ms (CPU)\n- Fusion: ~1ms\n- Total: ~1-1.5s per frame\n\n**Next:** Run notebook 05_full_analysis.ipynb for thesis-ready analysis"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06 - Florence-2 Visual Grounding Test\n",
    "\n",
    "Test the Florence-2 vision-language model for:\n",
    "- Scene captioning (\"What is in front of me?\")\n",
    "- Visual grounding (\"Where is my phone?\")\n",
    "- Object detection\n",
    "\n",
    "All functionality is in `src/` modules - this notebook only calls functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from IPython.display import display, Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.config import Config, FlorenceConfig\n",
    "from src.florence import FlorenceModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Florence-2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Florence-2\n",
    "florence_config = FlorenceConfig(\n",
    "    enabled=True,\n",
    "    model=\"microsoft/Florence-2-base\",  # Options: Florence-2-base, Florence-2-large\n",
    "    device=\"cpu\",  # Use \"cuda\" if GPU available\n",
    ")\n",
    "\n",
    "# Create and load model\n",
    "florence = FlorenceModel(florence_config)\n",
    "print(\"Loading Florence-2 model (this may take a minute)...\")\n",
    "success = florence.load()\n",
    "print(f\"Model loaded: {success}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Test Image\n",
    "\n",
    "Use a captured frame from the Pi camera or a sample indoor image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Load from captured frames\n",
    "import glob\n",
    "\n",
    "captured_frames = glob.glob('../data/captures/*.jpg')\n",
    "if captured_frames:\n",
    "    test_image_path = captured_frames[0]\n",
    "    print(f\"Using captured frame: {test_image_path}\")\n",
    "    test_image = cv2.imread(test_image_path)\n",
    "else:\n",
    "    # Option 2: Create a simple test image\n",
    "    print(\"No captured frames found, creating test image\")\n",
    "    test_image = np.zeros((480, 640, 3), dtype=np.uint8)\n",
    "    test_image[:] = (200, 200, 200)  # Gray background\n",
    "    # Draw some shapes to simulate objects\n",
    "    cv2.rectangle(test_image, (100, 100), (250, 350), (139, 69, 19), -1)  # Brown door\n",
    "    cv2.rectangle(test_image, (400, 200), (550, 400), (50, 50, 150), -1)  # Blue chair\n",
    "    cv2.circle(test_image, (300, 150), 30, (100, 100, 100), -1)  # Gray circle\n",
    "\n",
    "# Display the image\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(cv2.cvtColor(test_image, cv2.COLOR_BGR2RGB))\n",
    "plt.title(\"Test Image\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Image shape: {test_image.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Scene Captioning\n",
    "\n",
    "Answer: \"What is in front of me?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple caption\n",
    "print(\"Generating simple caption...\")\n",
    "simple_caption = florence.caption(test_image, detailed=False)\n",
    "print(f\"Simple Caption: {simple_caption}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Detailed caption\n",
    "print(\"Generating detailed caption...\")\n",
    "detailed_caption = florence.caption(test_image, detailed=True)\n",
    "print(f\"Detailed Caption: {detailed_caption}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Object Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect all objects\n",
    "print(\"Running object detection...\")\n",
    "detections = florence.detect(test_image)\n",
    "\n",
    "print(f\"\\nDetected {len(detections)} objects:\")\n",
    "for det in detections:\n",
    "    print(f\"  - {det.phrase}: bbox={det.bbox}, center={det.center}\")\n",
    "\n",
    "# Visualize\n",
    "result_img = florence.draw_results(test_image, detections)\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(cv2.cvtColor(result_img, cv2.COLOR_BGR2RGB))\n",
    "plt.title(f\"Object Detection ({len(detections)} objects)\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visual Grounding (Object Search)\n",
    "\n",
    "Answer: \"Where is my phone?\" / \"Find the door\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for specific objects\n",
    "search_targets = [\"door\", \"chair\", \"table\", \"phone\", \"person\"]\n",
    "\n",
    "for target in search_targets:\n",
    "    print(f\"\\nSearching for: '{target}'\")\n",
    "    results = florence.ground(test_image, target)\n",
    "    \n",
    "    if results:\n",
    "        print(f\"  Found {len(results)} match(es):\")\n",
    "        for r in results:\n",
    "            print(f\"    - {r.phrase}: bbox={r.bbox}, center={r.center}\")\n",
    "        \n",
    "        # Visualize first result\n",
    "        result_img = florence.draw_results(test_image, results)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.imshow(cv2.cvtColor(result_img, cv2.COLOR_BGR2RGB))\n",
    "        plt.title(f\"Found: {target}\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"  Not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test with Pipeline\n",
    "\n",
    "Test the full interactive pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.config import Config\n",
    "from src.pipeline import SmartAidPipeline\n",
    "\n",
    "# Configure pipeline (disable voice for notebook testing)\n",
    "config = Config()\n",
    "config.pipeline.use_voice_input = False\n",
    "config.pipeline.use_voice_output = False\n",
    "config.pipeline.use_florence = True\n",
    "config.pipeline.use_yolo_world = True\n",
    "config.pipeline.use_depth = True\n",
    "\n",
    "# Use the already-loaded Florence model's config\n",
    "config.florence = florence_config\n",
    "\n",
    "print(\"Creating pipeline...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and load pipeline\n",
    "pipeline = SmartAidPipeline(config)\n",
    "print(\"Loading pipeline components...\")\n",
    "pipeline.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test queries\n",
    "test_queries = [\n",
    "    \"What is in front of me?\",\n",
    "    \"Where is the door?\",\n",
    "    \"Find the chair\",\n",
    "    \"What objects are here?\",\n",
    "    \"How many chairs?\",\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Testing Interactive Queries\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\n> {query}\")\n",
    "    response = pipeline.process_query(query, test_image)\n",
    "    print(f\"< {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Performance Measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Measure caption time\n",
    "start = time.time()\n",
    "_ = florence.caption(test_image, detailed=True)\n",
    "caption_time = time.time() - start\n",
    "print(f\"Caption time: {caption_time:.2f}s\")\n",
    "\n",
    "# Measure detection time\n",
    "start = time.time()\n",
    "_ = florence.detect(test_image)\n",
    "detect_time = time.time() - start\n",
    "print(f\"Detection time: {detect_time:.2f}s\")\n",
    "\n",
    "# Measure grounding time\n",
    "start = time.time()\n",
    "_ = florence.ground(test_image, \"door\")\n",
    "ground_time = time.time() - start\n",
    "print(f\"Grounding time: {ground_time:.2f}s\")\n",
    "\n",
    "print(f\"\\nTotal estimated query time: {caption_time + detect_time + ground_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary\n",
    "\n",
    "Florence-2 provides:\n",
    "- **Captioning**: Describe scenes in natural language\n",
    "- **Object Detection**: Find all objects in an image\n",
    "- **Visual Grounding**: Locate specific objects by description\n",
    "\n",
    "This enables conversational queries like:\n",
    "- \"What is in front of me?\" → Uses captioning\n",
    "- \"Where is my phone?\" → Uses visual grounding\n",
    "- \"What objects are here?\" → Uses detection"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
